{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-03T12:42:10.706041Z","iopub.execute_input":"2022-06-03T12:42:10.706436Z","iopub.status.idle":"2022-06-03T12:42:10.713481Z","shell.execute_reply.started":"2022-06-03T12:42:10.706403Z","shell.execute_reply":"2022-06-03T12:42:10.712468Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# load the csv data\ndf = pd.read_csv('../input/creditcardfraud/creditcard.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T12:42:25.537152Z","iopub.execute_input":"2022-06-03T12:42:25.537560Z","iopub.status.idle":"2022-06-03T12:42:30.389153Z","shell.execute_reply.started":"2022-06-03T12:42:25.537528Z","shell.execute_reply":"2022-06-03T12:42:30.388192Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T12:46:09.278620Z","iopub.execute_input":"2022-06-03T12:46:09.279079Z","iopub.status.idle":"2022-06-03T12:46:09.787568Z","shell.execute_reply.started":"2022-06-03T12:46:09.279042Z","shell.execute_reply":"2022-06-03T12:46:09.786787Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T12:47:16.101552Z","iopub.execute_input":"2022-06-03T12:47:16.101980Z","iopub.status.idle":"2022-06-03T12:47:16.143479Z","shell.execute_reply.started":"2022-06-03T12:47:16.101947Z","shell.execute_reply":"2022-06-03T12:47:16.142290Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"****Preprocessing the dataset****","metadata":{}},{"cell_type":"code","source":"df.isnull().sum() #check for null values","metadata":{"execution":{"iopub.status.busy":"2022-06-03T12:49:12.354464Z","iopub.execute_input":"2022-06-03T12:49:12.354879Z","iopub.status.idle":"2022-06-03T12:49:12.381081Z","shell.execute_reply.started":"2022-06-03T12:49:12.354833Z","shell.execute_reply":"2022-06-03T12:49:12.380091Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"****Exploratory Data Analysis****","metadata":{}},{"cell_type":"code","source":"#Let us first explore the categorical column \"Class\".\nsns.countplot(df['Class'])","metadata":{"execution":{"iopub.status.busy":"2022-06-03T12:57:52.338219Z","iopub.execute_input":"2022-06-03T12:57:52.338634Z","iopub.status.idle":"2022-06-03T12:57:52.506342Z","shell.execute_reply.started":"2022-06-03T12:57:52.338602Z","shell.execute_reply":"2022-06-03T12:57:52.505379Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"**The number of fraudulent classes is low.****\n\n****Hence, we need to balance the data for reasonable results.**","metadata":{}},{"cell_type":"code","source":"#To display all the 28 PCA columns, we need to run a loop.\ndf_temp = df.drop(columns=['Time', 'Amount', 'Class'], axis=1)\n\n# create dist plots\nfig, ax = plt.subplots(ncols=4, nrows=7, figsize=(20, 50))\nindex = 0\nax = ax.flatten()\n\nfor col in df_temp.columns:\n    sns.distplot(df_temp[col], ax=ax[index])\n    index += 1\nplt.tight_layout(pad=0.5, w_pad=0.5, h_pad=5)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T13:06:40.128257Z","iopub.execute_input":"2022-06-03T13:06:40.128728Z","iopub.status.idle":"2022-06-03T13:07:18.971383Z","shell.execute_reply.started":"2022-06-03T13:06:40.128692Z","shell.execute_reply":"2022-06-03T13:07:18.970485Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#Let us explore the column \"Time\".\n\nsns.distplot(df['Time'])","metadata":{"execution":{"iopub.status.busy":"2022-06-03T13:12:30.911595Z","iopub.execute_input":"2022-06-03T13:12:30.911991Z","iopub.status.idle":"2022-06-03T13:12:32.220985Z","shell.execute_reply.started":"2022-06-03T13:12:30.911959Z","shell.execute_reply":"2022-06-03T13:12:32.219952Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#To display the column \"Amount\".\n\nsns.distplot(df['Amount'])\n","metadata":{"execution":{"iopub.status.busy":"2022-06-03T13:12:51.514652Z","iopub.execute_input":"2022-06-03T13:12:51.515080Z","iopub.status.idle":"2022-06-03T13:12:52.855735Z","shell.execute_reply.started":"2022-06-03T13:12:51.515045Z","shell.execute_reply":"2022-06-03T13:12:52.854844Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"****In this specific project, the correlation matrix is insignificant because of the lack of meaningful information. All the columns containing random pieces of information is dynamically reduced using PCA transformation.****","metadata":{}},{"cell_type":"markdown","source":"**Model training**","metadata":{}},{"cell_type":"code","source":"#Input Split\n\nX = df.drop(columns=['Class'], axis=1)\ny = df['Class']\n","metadata":{"execution":{"iopub.status.busy":"2022-06-03T13:18:47.603382Z","iopub.execute_input":"2022-06-03T13:18:47.603771Z","iopub.status.idle":"2022-06-03T13:18:47.639991Z","shell.execute_reply.started":"2022-06-03T13:18:47.603741Z","shell.execute_reply":"2022-06-03T13:18:47.638816Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"**Standard scaling for all variables except output class**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_scaler = sc.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T13:25:30.931743Z","iopub.execute_input":"2022-06-03T13:25:30.932163Z","iopub.status.idle":"2022-06-03T13:25:31.126048Z","shell.execute_reply.started":"2022-06-03T13:25:30.932131Z","shell.execute_reply":"2022-06-03T13:25:31.125011Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"x_scaler [-1] #to print","metadata":{"execution":{"iopub.status.busy":"2022-06-03T13:26:44.457601Z","iopub.execute_input":"2022-06-03T13:26:44.458026Z","iopub.status.idle":"2022-06-03T13:26:44.464721Z","shell.execute_reply.started":"2022-06-03T13:26:44.457991Z","shell.execute_reply":"2022-06-03T13:26:44.463904Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"**All Input attributes are in the X and y contains the output Class.\n\n****After running the code, we can see an array with a scaled value ranging from 0-1. ****\n\n****To understand the process, please go through the formula of Standard Scalar.**","metadata":{}},{"cell_type":"markdown","source":"****Model Training and Testing****","metadata":{}},{"cell_type":"code","source":"#Splitting the Data:\n\n# train test split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, f1_score\nx_train, x_test, y_train, y_test = train_test_split(x_scaler, y, test_size=0.25, random_state=42, stratify=y)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T13:33:12.686020Z","iopub.execute_input":"2022-06-03T13:33:12.686431Z","iopub.status.idle":"2022-06-03T13:33:12.989927Z","shell.execute_reply.started":"2022-06-03T13:33:12.686399Z","shell.execute_reply":"2022-06-03T13:33:12.988841Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"**We have to use stratify to uniformly distribute class variables (Because the class is not balanced).**","metadata":{}},{"cell_type":"code","source":"sns.distplot(df['Class'])","metadata":{"execution":{"iopub.status.busy":"2022-06-03T13:39:19.117841Z","iopub.execute_input":"2022-06-03T13:39:19.118272Z","iopub.status.idle":"2022-06-03T13:39:20.511829Z","shell.execute_reply.started":"2022-06-03T13:39:19.118239Z","shell.execute_reply":"2022-06-03T13:39:20.511036Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"#Logistic Regression:\n\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\n# training\nmodel.fit(x_train, y_train)\n# testing\ny_pred = model.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint(\"F1 Score:\",f1_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-03T13:39:42.973805Z","iopub.execute_input":"2022-06-03T13:39:42.974225Z","iopub.status.idle":"2022-06-03T13:39:46.104506Z","shell.execute_reply.started":"2022-06-03T13:39:42.974192Z","shell.execute_reply":"2022-06-03T13:39:46.100581Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"**Here, we can observe accuracy as 100% (Because of the Standard Scaling). However, the majority of the accuracy is based on Non-Fraudulent samples.****\n\n****F1-Score is a combination of Precision and Recall.****\n\n****Since the F1 score is around 72%, we have to consider a better Model for training.**","metadata":{}},{"cell_type":"code","source":"#Random Forest:\n\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\n# training\nmodel.fit(x_train, y_train)\n# testing\ny_pred = model.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint(\"F1 Score:\",f1_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-03T13:47:43.320706Z","iopub.execute_input":"2022-06-03T13:47:43.321141Z","iopub.status.idle":"2022-06-03T13:51:52.251946Z","shell.execute_reply.started":"2022-06-03T13:47:43.321108Z","shell.execute_reply":"2022-06-03T13:51:52.250845Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"**After running the code, we have to wait longer than usual due to the larger number of Data-Set values.****\n\n****Now the F1-Score has improved.**** \n\n****Due to unbalanced training, we are observing a low score.****\n\n****Let us try one boosting model.****\n\n**","metadata":{}},{"cell_type":"code","source":"#XGBoost:\n\nfrom xgboost import XGBClassifier\nmodel = XGBClassifier(n_jobs=-1)\n# training\nmodel.fit(x_train, y_train)\n# testing\ny_pred = model.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint(\"F1 Score:\",f1_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-03T14:02:27.502080Z","iopub.execute_input":"2022-06-03T14:02:27.502452Z","iopub.status.idle":"2022-06-03T14:03:26.138052Z","shell.execute_reply.started":"2022-06-03T14:02:27.502425Z","shell.execute_reply":"2022-06-03T14:03:26.136973Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**We can observe an F1-Score of 86%, which is a good result.****\n\n****However, let us try to balance this data and see if the results improve in terms of F1-Score and Macro Average.**","metadata":{}},{"cell_type":"markdown","source":"****Balancing the classes using SMOTE****\n\n****We will now balance the class with equal distribution and train them with similar models.\nBefore that, let us see the class ratio.****","metadata":{}},{"cell_type":"code","source":"#The difference between 0 and 1 classes is large.\n\nsns.countplot(y_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T14:04:50.351844Z","iopub.execute_input":"2022-06-03T14:04:50.352232Z","iopub.status.idle":"2022-06-03T14:04:50.537379Z","shell.execute_reply.started":"2022-06-03T14:04:50.352202Z","shell.execute_reply":"2022-06-03T14:04:50.536259Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"**Class Imbalancement:**","metadata":{}},{"cell_type":"code","source":"# balance the class with equal distribution\nfrom imblearn.over_sampling import SMOTE\nover_sample = SMOTE()\nx_smote, y_smote = over_sample.fit_resample(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T14:05:07.446193Z","iopub.execute_input":"2022-06-03T14:05:07.446575Z","iopub.status.idle":"2022-06-03T14:05:07.848699Z","shell.execute_reply.started":"2022-06-03T14:05:07.446547Z","shell.execute_reply":"2022-06-03T14:05:07.847672Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"****We can use Random Under_Sampling to reduce the data and Random Over_Sampling for increasing the data.**** \n\n****The use of these balancing methods will result in good values.**","metadata":{}},{"cell_type":"code","source":"sns.countplot(y_smote)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T14:05:59.862473Z","iopub.execute_input":"2022-06-03T14:05:59.862916Z","iopub.status.idle":"2022-06-03T14:06:00.084613Z","shell.execute_reply.started":"2022-06-03T14:05:59.862855Z","shell.execute_reply":"2022-06-03T14:06:00.083588Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"**Now the sample is equally distributed, the model will give weightage for both of these classes.**","metadata":{}},{"cell_type":"code","source":"#XGBoost again:\n\nfrom xgboost import XGBClassifier\nmodel = XGBClassifier(n_jobs=-1)\n# training\nmodel.fit(x_smote, y_smote)\n# testing\ny_pred = model.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint(\"F1 Score:\",f1_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-03T14:10:19.787451Z","iopub.execute_input":"2022-06-03T14:10:19.787855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**since we did'nt got better f1 score we will stick to imbalanced data in this one**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}